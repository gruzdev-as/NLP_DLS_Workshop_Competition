{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from tqdm.notebook import tqdm \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы логгировать результаты модели в wandb необходимо передать свой API ключ. Давать его в коде не лучая практика, можно тянуть из конфига, но на кагле было немного лениво этим заморачиваться. \n",
    "\n",
    "Доступ к модели Mistral выдается после принятия лицензионного соглашения, поэтому также понадобится ключи и от HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фиксируем сиды\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='YOUR_WANDB_API_KEY')\n",
    "login(token = 'YOUR_HF_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства задаем путь к папке с данными. Оставил тут как это было у меня в Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/kaggle/input/dls-nlp-workshop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH / 'train.csv.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile, delimiter=','))\n",
    "    header_row = data.pop(0)\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "idx, text, labels = list(zip(*[(int(row[0]), f'tags: {row[3].strip()}\\n\\nReview: {row[4].strip()}', row[5:]) for row in data]))\n",
    "labels = np.array(labels, dtype=int)\n",
    "\n",
    "# Веса классов в теории могут помочь при дисбалансе \n",
    "label_weights = 1 - labels.sum(axis=0) / labels.sum()\n",
    "\n",
    "row_ids = np.arange(len(labels))\n",
    "train_idx, y_train, val_idx, y_val = iterative_train_test_split(row_ids[:,np.newaxis], labels, test_size = 0.1)\n",
    "x_train = [text[i] for i in train_idx.flatten()]\n",
    "x_val = [text[i] for i in val_idx.flatten()]\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': x_train, 'labels': y_train}),\n",
    "    'val': Dataset.from_dict({'text': x_val, 'labels': y_val})\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_examples(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['text'])\n",
    "    tokenized_inputs['labels'] = examples['labels']\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6201cf390b31411fa3fa54f75b29a257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e08c0d62b884fbfa504f9b899c22d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1467e37823f42028a721a7a5aaf1920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb8e00f19894fe1804c68848352353f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d020080d8acd42919ebe51eff77a4ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a5501a41f04339bcfe7f7f16949b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065cd4164deb451bad3a7ae60f0d2a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffd499ab59844cf8172c10039c37126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3a8d04375f4f4dbc71f85e6ac27f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac5931d9b3547598576a9f9addb8a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b24adae4054cee95df8ac5502cf3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1240fa2281046c8b304279ed86b9c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\n",
    "tokenized_ds = tokenized_ds.with_format('torch')\n",
    "\n",
    "# Квантизуем в 4бит по гайду\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "# Параметры конфига из гайда \n",
    "lora_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=labels.shape[1]\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кастомная функция для препроцессинга батча\n",
    "def collate_fn(batch, tokenizer):\n",
    "    dict_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n",
    "    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['attention_mask'], batch_first=True, padding_value=0\n",
    "    )\n",
    "    d['labels'] = torch.stack(d['labels'])\n",
    "    return d\n",
    "\n",
    "# Метрики которые будем отслеживать на валидации\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    f1_micro = f1_score(labels, predictions > 0, average = 'micro')\n",
    "    f1_macro = f1_score(labels, predictions > 0, average = 'macro')\n",
    "    f1_weighted = f1_score(labels, predictions > 0, average = 'weighted')\n",
    "    accuracy = accuracy_score(labels, predictions > 0)\n",
    "    return {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, label_weights, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.label_weights = label_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.to(torch.float32), pos_weight=self.label_weights)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstrangerone\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241002_003149-zdusa2sz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmultilabel_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/strangerone/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/strangerone/huggingface/runs/zdusa2sz\u001b[0m\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1554' max='1554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1554/1554 5:51:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>0.551196</td>\n",
       "      <td>0.192709</td>\n",
       "      <td>0.486737</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.043728</td>\n",
       "      <td>0.672370</td>\n",
       "      <td>0.396372</td>\n",
       "      <td>0.635028</td>\n",
       "      <td>0.456250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.041099</td>\n",
       "      <td>0.725962</td>\n",
       "      <td>0.475849</td>\n",
       "      <td>0.704479</td>\n",
       "      <td>0.512500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1554, training_loss=0.0696430795441263, metrics={'train_runtime': 21091.718, 'train_samples_per_second': 0.589, 'train_steps_per_second': 0.074, 'total_flos': 5.775669079444685e+16, 'train_loss': 0.0696430795441263, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'multilabel_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = functools.partial(collate_fn, tokenizer=tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    "    label_weights = torch.tensor(label_weights, device=model.device)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('multilabel_mistral/tokenizer_config.json',\n",
       " 'multilabel_mistral/special_tokens_map.json',\n",
       " 'multilabel_mistral/tokenizer.model',\n",
       " 'multilabel_mistral/added_tokens.json',\n",
       " 'multilabel_mistral/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Сохраняем модель\n",
    "peft_model_id = 'multilabel_mistral'\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme here\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "# Грузим сохраненную модель\n",
    "base_model_id = '/kaggle/input/mistral-3-epochs-model/multilabel_mistral'\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_id, \n",
    "    quantization_config = quantization_config,\n",
    "    num_labels=50)\n",
    "\n",
    "peft_model_id = '/kaggle/input/mistral-3-epochs-model/multilabel_mistral'\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH / '/test.csv.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile, delimiter=','))\n",
    "    header_row = data.pop(0)\n",
    "\n",
    "idx, text = list(zip(*[(int(row[0]), f'tags: {row[3].strip()}\\n\\nReview: {row[4].strip()}') for row in data]))\n",
    "texts = [i for i in text]\n",
    "\n",
    "dataset = TextDataset(texts)\n",
    "batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "all_logits = []\n",
    "\n",
    "for batch_texts in tqdm(dataloader):\n",
    "    tokenized_inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_inputs = {k: v.to('cuda') for k, v in tokenized_inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_inputs).logits\n",
    "        all_logits.append(outputs.cpu())\n",
    "\n",
    "all_logits = torch.cat(all_logits, dim=0)\n",
    "probs = torch.sigmoid(all_logits).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "threshold = 0.6 # ВЫБРАТЬ НА КРОСС_ВАЛИДАЦИИ\n",
    "\n",
    "def clear(row):\n",
    "    row = row[1:-1]\n",
    "    row = row.replace(',', ' ')\n",
    "    return row.strip()\n",
    "\n",
    "for i in np.ndindex(probs.shape[0]):\n",
    "    row_indices = np.where(probs[i] > threshold)[0]\n",
    "    predictions.append(row_indices)\n",
    "\n",
    "\n",
    "sub = pd.DataFrame({'index':idx, 'target':[clear(str(i)) for i in predictions]})\n",
    "sub.to_csv(f'submission_mistral_{threshold}.csv', index=False)\n",
    "np.savetxt(f\"probs_mistral_{threshold}.csv\", probs, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
