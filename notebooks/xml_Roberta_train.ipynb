{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9550653,"sourceType":"datasetVersion","datasetId":5772156}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\nimport torch\nimport torch.nn.functional as F\nfrom transformers import TrainerCallback\nimport logging\nimport numpy as np\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:56.866285Z","iopub.execute_input":"2024-10-08T14:02:56.866722Z","iopub.status.idle":"2024-10-08T14:02:56.872849Z","shell.execute_reply.started":"2024-10-08T14:02:56.866669Z","shell.execute_reply":"2024-10-08T14:02:56.871869Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# –ö–∞—Å—Ç–æ–º–Ω—ã–π –∫–æ–ª–ª–±–µ–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\nclass CustomCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n        logging.info(f\"Epoch {state.epoch}: {logs}\")\n        print(f\"Epoch {state.epoch}: {logs}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:56.874415Z","iopub.execute_input":"2024-10-08T14:02:56.874744Z","iopub.status.idle":"2024-10-08T14:02:56.888922Z","shell.execute_reply.started":"2024-10-08T14:02:56.874686Z","shell.execute_reply":"2024-10-08T14:02:56.888004Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = Path('/kaggle/input/dls-nlp-workshop/NLP DATA')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:56.890775Z","iopub.execute_input":"2024-10-08T14:02:56.891476Z","iopub.status.idle":"2024-10-08T14:02:56.902359Z","shell.execute_reply.started":"2024-10-08T14:02:56.891410Z","shell.execute_reply":"2024-10-08T14:02:56.901578Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(DATA_PATH / 'train.csv.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:56.903382Z","iopub.execute_input":"2024-10-08T14:02:56.903651Z","iopub.status.idle":"2024-10-08T14:02:56.960494Z","shell.execute_reply.started":"2024-10-08T14:02:56.903622Z","shell.execute_reply":"2024-10-08T14:02:56.959737Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data['tags'] = data['tags'].fillna('')\ndata['input_text'] = data['tags'] + \" \" + data['text'].fillna('').apply(lambda x: x.lower().strip())\nlabels = data[[f'trend_id_res{i}' for i in range(50)]].values  # Assuming 50 classes","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:56.962644Z","iopub.execute_input":"2024-10-08T14:02:56.963028Z","iopub.status.idle":"2024-10-08T14:02:56.982533Z","shell.execute_reply.started":"2024-10-08T14:02:56.962992Z","shell.execute_reply":"2024-10-08T14:02:56.981636Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['input_text'], labels, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:56.983901Z","iopub.execute_input":"2024-10-08T14:02:56.984279Z","iopub.status.idle":"2024-10-08T14:02:56.994728Z","shell.execute_reply.started":"2024-10-08T14:02:56.984208Z","shell.execute_reply":"2024-10-08T14:02:56.993937Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:56.995956Z","iopub.execute_input":"2024-10-08T14:02:56.996328Z","iopub.status.idle":"2024-10-08T14:02:59.794619Z","shell.execute_reply.started":"2024-10-08T14:02:56.996296Z","shell.execute_reply":"2024-10-08T14:02:59.793567Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.DataFrame({'text': X_train.tolist(), 'labels': list(y_train)})\ntest_data = pd.DataFrame({'text': X_test.tolist(), 'labels': list(y_test)})\n\ntrain_dataset = Dataset.from_pandas(train_data)\ntest_dataset = Dataset.from_pandas(test_data)\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:02:59.795818Z","iopub.execute_input":"2024-10-08T14:02:59.796131Z","iopub.status.idle":"2024-10-08T14:03:01.891624Z","shell.execute_reply.started":"2024-10-08T14:02:59.796098Z","shell.execute_reply":"2024-10-08T14:03:01.890543Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3698 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9699f4fe532a48ab9bc0e1194226f715"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/925 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6205711b7d433f80364bf805fae31b"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = train_dataset.remove_columns(['text'])\ntest_dataset = test_dataset.remove_columns(['text'])\ntrain_dataset.set_format('torch')\ntest_dataset.set_format('torch')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:01.893365Z","iopub.execute_input":"2024-10-08T14:03:01.893759Z","iopub.status.idle":"2024-10-08T14:03:01.904896Z","shell.execute_reply.started":"2024-10-08T14:03:01.893716Z","shell.execute_reply":"2024-10-08T14:03:01.903684Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=50)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:01.905969Z","iopub.execute_input":"2024-10-08T14:03:01.906352Z","iopub.status.idle":"2024-10-08T14:03:02.199182Z","shell.execute_reply.started":"2024-10-08T14:03:01.906312Z","shell.execute_reply":"2024-10-08T14:03:02.198210Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def __init__(self, label_weights, **kwargs):\n        super().__init__(**kwargs)\n        self.label_weights = label_weights\n    \n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop('labels')\n        \n        # Move labels and inputs to the same device as the model\n        labels = labels.to(model.device)\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n        outputs = model(**inputs)\n        logits = outputs.get('logits')\n        \n        # Custom loss function with label weights\n        loss = F.binary_cross_entropy_with_logits(\n            logits, labels.to(torch.float32), pos_weight=self.label_weights\n        )\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:02.201909Z","iopub.execute_input":"2024-10-08T14:03:02.202223Z","iopub.status.idle":"2024-10-08T14:03:02.209630Z","shell.execute_reply.started":"2024-10-08T14:03:02.202189Z","shell.execute_reply":"2024-10-08T14:03:02.208580Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"label_weights = 1 - labels.sum(axis=0) / labels.sum()\nlabel_weights = torch.from_numpy(label_weights).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nlabel_weights","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:02.211172Z","iopub.execute_input":"2024-10-08T14:03:02.212216Z","iopub.status.idle":"2024-10-08T14:03:02.228093Z","shell.execute_reply.started":"2024-10-08T14:03:02.212180Z","shell.execute_reply":"2024-10-08T14:03:02.227144Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"tensor([0.8761, 0.9486, 0.9116, 0.9466, 0.9813, 0.9923, 0.9969, 0.9948, 0.9793,\n        0.9985, 0.9858, 0.9840, 0.9076, 0.9950, 0.9892, 0.9880, 0.9717, 0.9987,\n        0.9658, 0.9467, 0.9805, 0.9853, 0.9963, 0.9904, 0.9985, 0.9981, 0.9960,\n        0.9216, 0.9378, 0.9910, 0.9547, 0.9926, 0.9981, 0.9959, 0.9984, 0.9914,\n        0.9753, 0.9932, 0.9959, 0.9935, 0.9893, 0.9969, 0.9948, 0.9935, 0.9960,\n        0.9982, 0.9990, 0.9954, 0.9957, 0.9982], device='cuda:0',\n       dtype=torch.float64)"},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(p):\n    predictions, labels = p\n    f1_micro = f1_score(labels, predictions > 0, average = 'micro')\n    f1_macro = f1_score(labels, predictions > 0, average = 'macro')\n    f1_weighted = f1_score(labels, predictions > 0, average = 'weighted')\n    accuracy = accuracy_score(labels, predictions > 0)\n    return {\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'f1_weighted': f1_weighted,\n        'accuracy': accuracy\n    }","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:02.229240Z","iopub.execute_input":"2024-10-08T14:03:02.229609Z","iopub.status.idle":"2024-10-08T14:03:02.235930Z","shell.execute_reply.started":"2024-10-08T14:03:02.229560Z","shell.execute_reply":"2024-10-08T14:03:02.234991Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',          \n    report_to='none',\n    evaluation_strategy=\"epoch\",     \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,   \n    num_train_epochs=3,              \n    weight_decay=0.01,               \n    logging_dir='./logs',            \n    logging_steps=10,\n    save_steps=500,                  \n    save_total_limit=2,              \n    metric_for_best_model=\"accuracy\", \n    logging_strategy=\"epoch\",        \n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:02.237036Z","iopub.execute_input":"2024-10-08T14:03:02.237322Z","iopub.status.idle":"2024-10-08T14:03:02.268538Z","shell.execute_reply.started":"2024-10-08T14:03:02.237291Z","shell.execute_reply":"2024-10-08T14:03:02.267640Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,                          \n    args=training_args,                   \n    train_dataset=train_dataset,          \n    eval_dataset=test_dataset,            \n    compute_metrics=compute_metrics,      \n    label_weights=label_weights,          \n    callbacks=[CustomCallback()]          \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:02.269888Z","iopub.execute_input":"2024-10-08T14:03:02.270553Z","iopub.status.idle":"2024-10-08T14:03:02.653653Z","shell.execute_reply.started":"2024-10-08T14:03:02.270499Z","shell.execute_reply":"2024-10-08T14:03:02.652858Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T14:03:02.654864Z","iopub.execute_input":"2024-10-08T14:03:02.655178Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='521' max='696' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [521/696 08:18 < 02:48, 1.04 it/s, Epoch 2.24/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>F1 Weighted</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.168700</td>\n      <td>0.104126</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.109400</td>\n      <td>0.102929</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Epoch 1.0: None\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2.0: None\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}